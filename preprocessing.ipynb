{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分类统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(79676, 3)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "tr_data_path = 'train.json'\n",
    "data = pd.read_json(tr_data_path)\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6743310407148952 0.00931271650183243\n"
     ]
    }
   ],
   "source": [
    "count_res = Counter(data['action_cause'])\n",
    "freq_res = sorted([(x[0], x[1] / 79676) for x in list(count_res.items())], key=lambda x: x[1], reverse=True)\n",
    "top_freq = sum([x[1] for x in freq_res[:10]])\n",
    "tail_freq = sum([x[1] for x in freq_res[-50:]])\n",
    "print(top_freq, tail_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8046965209096841 0.00719162608564687\n"
     ]
    }
   ],
   "source": [
    "criminal_data_path = '../criminal/train.json'\n",
    "data_cri = pd.read_json(criminal_data_path)\n",
    "data_cri['first_charge'] = data_cri['charge'].apply(lambda x: x[0])\n",
    "count_res = Counter(data_cri['first_charge'])\n",
    "freq_res = sorted([(x[0], x[1] / 79676) for x in list(count_res.items())], key=lambda x: x[1], reverse=True)\n",
    "top_freq = sum([x[1] for x in freq_res[:10]])\n",
    "tail_freq = sum([x[1] for x in freq_res[-50:]])\n",
    "print(top_freq, tail_freq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并生成字典"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58350\n"
     ]
    }
   ],
   "source": [
    "file_list = ['D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\THUOCL_caijing.txt',\n",
    "             'D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\THUOCL_diming.txt',\n",
    "             'D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\THUOCL_law.txt']\n",
    "\n",
    "word_set = set()\n",
    "for current_file in file_list:\n",
    "    with open(current_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            item = line.split('\\t')[0].strip()\n",
    "            word_set.add(item)\n",
    "\n",
    "word_set.remove('')\n",
    "\n",
    "print(len(word_set))\n",
    "new_file = 'D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\user_dict.txt'\n",
    "with open(new_file, 'w', encoding='utf-8') as file:\n",
    "    for word in word_set:\n",
    "        file.write(word+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 合并停用词表"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317\n"
     ]
    }
   ],
   "source": [
    "stopword_list = ['D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\baidu_stopwords.txt',\n",
    "             'D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\cn_stopwords.txt',\n",
    "             'D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\scu_stopwords.txt',\n",
    "             'D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\hit_stopwords.txt']\n",
    "\n",
    "word_set = set()\n",
    "for current_file in stopword_list:\n",
    "    with open(current_file, 'r', encoding='utf-8') as file:\n",
    "        for line in file.readlines():\n",
    "            word_set.add(line)\n",
    "\n",
    "print(len(word_set))\n",
    "new_file = 'D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\user_stopword.txt'\n",
    "with open(new_file, 'w', encoding='utf-8') as file:\n",
    "    for word in word_set:\n",
    "        file.write(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fact</th>\n",
       "      <th>laws</th>\n",
       "      <th>action_cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>原告诉称，2017年8月27日，三原告亲属杨明红（已死亡，身份证号码：）因子宫肌瘤到被告处就...</td>\n",
       "      <td>[[{'title': '中华人民共和国民法通则', 'date': {'year': 20...</td>\n",
       "      <td>医疗损害责任纠纷</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                fact  \\\n",
       "0  原告诉称，2017年8月27日，三原告亲属杨明红（已死亡，身份证号码：）因子宫肌瘤到被告处就...   \n",
       "\n",
       "                                                laws action_cause  \n",
       "0  [[{'title': '中华人民共和国民法通则', 'date': {'year': 20...     医疗损害责任纠纷  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "\n",
    "tr_data_path = 'train.json'\n",
    "data = pd.read_json(tr_data_path)\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    [原告, 诉称, ，, 2017年, 8月, 27日, ，, 三, 原告, 亲属, 杨, 明...\n",
       "1    [原告, 诉称, ，, 1999年, 底, ，, 原, 、, 被告, 经人, 介绍, 相识,...\n",
       "2    [原告, （, 反诉, 被告, ）, 钱, 洪, 、, 程, 玲玲, 向, 本院, 在, 本...\n",
       "3    [原告, 中国农业银行, 股份有限公司, 弥勒市, 支行, 诉称, ：, 2013年, 9月...\n",
       "4    [许葵, 向, 本院, 提出, 诉讼, 请求, ：, 1, 、, 判令, 被告, 立即, 偿...\n",
       "Name: words, dtype: object"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 对于train文件大概需要40分钟进行分词\n",
    "from pkuseg import pkuseg\n",
    "seg_model = pkuseg(user_dict='D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\user_dict.txt')\n",
    "data['words'] = data['fact'].apply(lambda x: seg_model.cut(x))\n",
    "data['words'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_4320/2431705767.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mlow_freq\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mword_frequency\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfreq\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mlow_freq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "# 统计出低频词并查看高频词\n",
    "# 713mins \n",
    "from collections import Counter\n",
    "word_frequency = dict(Counter(sum(data['words'],[])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "476453\n",
      "['￥￥', '￥．930000', '￥．8500', '￥．698000', '￥．668000', '￥﹤', '￥陆', '￥贰拾万', '￥贰', '￥肆']\n",
      "['2147.20', '东500', '吴展勇', '汪涛负', '8011772', '冀雅', '65284', '任玉婷', '232806', '141690', '东德信', '欣春苑', '酶Ⅲ', '24364', '慰斗厂', '怀望', '79649.66', '输入端', '斛圩', '兰传']\n"
     ]
    }
   ],
   "source": [
    "low_freq = set()\n",
    "for (word, freq) in zip(word_frequency.keys(), word_frequency.values()):\n",
    "    if freq <= 3:\n",
    "        low_freq.add(word)\n",
    "print(len(low_freq))\n",
    "\n",
    "word_freq_sorted = sorted(word_frequency,reverse=True)\n",
    "print(word_freq_sorted[:10])\n",
    "print(list(low_freq)[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 停用词与低频词过滤\n",
    "stopword_dict = set()\n",
    "with open('D:\\\\sjx\\\\RUC研究生\\\\毕业论文\\\\code\\\\dicts\\\\user_stopword.txt', 'r', encoding='utf-8') as file:\n",
    "    for line in file.readlines():\n",
    "        stopword_dict.add(line)\n",
    "\n",
    "def need_to_filter(item):\n",
    "    return item in stopword_dict or item in low_freq\n",
    "data['words_filtered'] = data['words'].apply(lambda x: [item for item in x if (not need_to_filter(item))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def illegal_char(item):\n",
    "    if len(item) == 0:\n",
    "        return True\n",
    "    for char in item:\n",
    "        if char.isdigit() or not ('\\u4e00' <= char <= '\\u9fff'):\n",
    "            return True\n",
    "    return False\n",
    "data['words_filtered_nums'] = data['words_filtered'].apply(lambda x: [item for item in x if (not illegal_char(item))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['input_sentence'] = data['words_filtered_nums'].apply(lambda x: \" \".join(x))\n",
    "data.loc[:,['input_sentence','action_cause']] .to_csv('./data/train_wordseq.csv', sep = '|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 整理为读入模型的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_sentence</th>\n",
       "      <th>action_cause</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>原告 诉称 三 原告 亲属 杨 明红 已 死亡 身份证 号码 因 子宫 肌瘤 到 被告处 就...</td>\n",
       "      <td>医疗损害责任纠纷</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      input_sentence action_cause\n",
       "0  原告 诉称 三 原告 亲属 杨 明红 已 死亡 身份证 号码 因 子宫 肌瘤 到 被告处 就...     医疗损害责任纠纷"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('./data/train_wordseq.csv', encoding='utf-8', sep = '|')\n",
    "data = data[['input_sentence','action_cause']]\n",
    "data.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    79676.000000\n",
       "mean       544.263454\n",
       "std        500.104900\n",
       "min          9.000000\n",
       "25%        236.000000\n",
       "50%        404.000000\n",
       "75%        682.000000\n",
       "max      15677.000000\n",
       "Name: sentence_length, dtype: float64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['sentence_length'] = data['input_sentence'].apply(lambda x: len(x.split(' ')))\n",
    "data['sentence_length'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "民间借贷纠纷                    17031\n",
       "机动车交通事故责任纠纷                9079\n",
       "金融借款合同纠纷                   6461\n",
       "离婚纠纷                       6253\n",
       "买卖合同纠纷                     6226\n",
       "                          ...  \n",
       "监护权纠纷                        12\n",
       "建设工程监理合同纠纷                   12\n",
       "土地承包经营权互换合同纠纷                12\n",
       "船舶抵押合同纠纷                     12\n",
       "认定公民无民事行为能力、限制民事行为能力案件       11\n",
       "Name: action_cause, Length: 257, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['action_cause'].value_counts()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "efbb3fc61f9fbd137dcdc26bc0e01e2d5f059e6c87b0cfa0c57cd44d3e2016ef"
  },
  "kernelspec": {
   "display_name": "Python 3.7.9 64-bit ('jiaxin': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
